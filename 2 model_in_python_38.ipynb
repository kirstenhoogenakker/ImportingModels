{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from dataikuapi.dss.ml import DSSPredictionMLTaskSettings\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these constants by your own values\n",
    "XP_TRACKING_FOLDER_ID = \"SVKZNnoa\"\n",
    "MLFLOW_EXPERIMENT_NAME = \"Python38_Models\"\n",
    "MLFLOW_CODE_ENV_NAME = \"py_36_pmp\"\n",
    "SAVED_MODEL_NAME = \"uci-bank-clf\"\n",
    "EVALUATION_DATASET = \"uci_bank_evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utils\n",
    "def now_str() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment tracking (scikit-learn)\n",
    "\n",
    "This notebook contains a simple example to showcase the new Experiment Tracking capabilities of Dataiku. It explains how to perform several runs with different parameters, select the best run and promote it as a Saved Model version in a Dataiku Flow. It leverages:\n",
    "* the [scikit-learn]() package\n",
    "* the [UCI Bank Marketing dataset]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the training data\n",
    "\n",
    "Our training data lives in the `uci_bank_train` Dataset, let's load it in a pandas DataFrame and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dataiku.api_client()\n",
    "project = client.get_default_project()\n",
    "training_dataset = dataiku.Dataset(\"uci_bank_train\")\n",
    "df = training_dataset.get_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working on a * binary classification* problem here, which is to predict whether or not a given person who was part of a marketing campaign ended up purchasing one of the bank's products. This outcome is reflected by the `y` column which can either take the \"no\" or \"yes\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"y\"\n",
    "target = df[target_name]\n",
    "data = df.drop(columns=[target_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the experiment\n",
    "\n",
    "To prepare the grounds for our experiments, we need to create a few handles and define which MLFlow experiment we'll collect our runs into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mlflow_extension object to easily collect information for the promotion step\n",
    "mlflow_extension = project.get_mlflow_extension()\n",
    "\n",
    "# Get a handle on the Managed Folder that will contain the experiment run + artifact data\n",
    "# TODO Get-or-create managed folder\n",
    "folder = project.get_managed_folder(odb_id=XP_TRACKING_FOLDER_ID)\n",
    "\n",
    "# Create a handle for the mlflow client\n",
    "mlflow_handle = project.setup_mlflow(managed_folder=folder)\n",
    "\n",
    "# Set the experiment\n",
    "mlflow.set_experiment(experiment_name=MLFLOW_EXPERIMENT_NAME)\n",
    "mlflow_experiment = mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimenting\n",
    "\n",
    "The goal of experiment tracking is to *instrument the iterative process of ML model training* by collecting all parameters and results of each trial. To be more specific, within an **experiment**, you perform multiple **runs**, each run being different from the others because of the **parameters** you use for it. You also need to specific which **metrics** to track, they will reflect the performance of the model for a given set of parameters.\n",
    "\n",
    "In this notebook example, if you want to produce experiment runs:\n",
    "* edit the parameters in the 3.1 cell and run it\n",
    "* run the 3.2 cell to effectively... perform the run ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Defining the parameters of our run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create run name\n",
    "run_name = f\"run-{now_str()}\"\n",
    "run_params = {}\n",
    "run_metrics = {}\n",
    "\n",
    "# Define run parameters\n",
    "# -- Which categorical columns to retain ?\n",
    "categorical_cols = [\"job\",\n",
    "                    \"marital\",\n",
    "                    \"education\",\n",
    "                    \"default\",\n",
    "                    \"housing\",\n",
    "                    \"loan\",\n",
    "                    \"contact\",\n",
    "                    \"month\",\n",
    "                    \"poutcome\"]\n",
    "run_params[\"categorical_cols\"] = categorical_cols\n",
    "\n",
    "# --Which algorithm to use? Which hyperparameters for this algo to try?\n",
    "# --- Example: Random Forest\n",
    "hparams = {\"n_estimators\": 100,\n",
    "           \"criterion\": \"gini\",\n",
    "           \"max_depth\": 6,\n",
    "           \"min_samples_split\": 5,\n",
    "           \"random_state\": 42}\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\"\"\"\n",
    "grid_rf_model = GridSearchCV(rf, parameters, cv=3)\n",
    "grid_rf_model.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_rf_model.best_estimator_\n",
    "for p in parameters:\n",
    "  print(\"Best '{}': {}\".format(p, best_rf.get_params()[p]))\n",
    "\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "clf = GridSearchCV(rf, hparams, cv=3)\n",
    "\"\"\"\n",
    "\n",
    "# ---Example: Gradient Boosting\n",
    "#hparams = {\"n_estimators\": 300,\n",
    "#           \"loss\": \"exponential\",\n",
    "#           \"learning_rate\": 0.1,\n",
    "#           \"max_depth\": 3,\n",
    "#           \"random_state\": 42}\n",
    "#clf = GradientBoostingClassifier(**hparams)\n",
    "model_algo = type(clf).__name__\n",
    "run_params[\"model_algo\"] = model_algo\n",
    "for hp in hparams.keys():\n",
    "    run_params[hp] = hparams[hp]\n",
    "\n",
    "# --Which cross-validation settings to use?\n",
    "n_cv_folds = 3\n",
    "cv = StratifiedKFold(n_splits=n_cv_folds)\n",
    "run_params[\"n_cv_folds\"] = n_cv_folds\n",
    "metrics = [\"f1_macro\", \"roc_auc\"]\n",
    "\n",
    "# --Let's print all of that to get a recap:\n",
    "print(f\"Parameters to log:\\n {run_params}\")\n",
    "print(100*'-')\n",
    "print(f\"Metrics to log:\\n {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Performing the run and logging parameters, metrics and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Starting run {run_name} (id: {run_id})...\")\n",
    "    # --Preprocessing\n",
    "    categorical_preprocessor = OrdinalEncoder()\n",
    "    preprocessor = ColumnTransformer([('categorical', categorical_preprocessor, categorical_cols)],\n",
    "                                     remainder=\"passthrough\")\n",
    "\n",
    "    # --Pipeline definition (preprocessing + model)\n",
    "    pipeline = make_pipeline(preprocessor, clf)\n",
    "\n",
    "    # --Cross-validation\n",
    "    print(f\"Running cross-validation...\")\n",
    "    scores = cross_validate(pipeline, data, target, cv=cv, scoring=metrics)\n",
    "    for m in [f\"test_{mname}\" for mname in metrics]:\n",
    "        run_metrics[f\"mean_{m}\"] = scores[m].mean()\n",
    "        run_metrics[f\"std_{m}\"] = scores[m].std()\n",
    "\n",
    "    # --Pipeline fit\n",
    "    pipeline.fit(X=data, y=target)\n",
    "    # --Log the order of the class label\n",
    "    run_params[\"class_labels\"] = pipeline.classes_.tolist()\n",
    "\n",
    "    # --Log parameters, metrics and model\n",
    "    mlflow.log_params(params=run_params)\n",
    "    mlflow.log_metrics(metrics=run_metrics)\n",
    "    artifact_path = f\"{model_algo}-{run_id}\"\n",
    "    mlflow.sklearn.log_model(sk_model=pipeline, artifact_path=artifact_path)\n",
    "\n",
    "    # --Set useful information to faciliate run promotion\n",
    "    mlflow_extension.set_run_inference_info(run_id=run_id,\n",
    "                                            prediction_type=\"BINARY_CLASSIFICATION\",\n",
    "                                            classes=run_params[\"class_labels\"],\n",
    "                                            code_env_name=MLFLOW_EXPERIMENT_NAME,\n",
    "                                            target=\"y\")\n",
    "    print(f\"DONE! Your artifacts are available at {run.info.artifact_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Promote a run to a Saved Model version\n",
    "\n",
    "Now that you have tried several parameters and performed multiple runs, you may want to choose the \"best\" one and actually surface it on your Dataiku Flow. This is done by *promoting* an experiment run into a Saved Model version. Let's start by figuring out which experiment got the best results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all runs of the experiment with performance metrics\n",
    "run_cv_metrics = {}\n",
    "for run_info in mlflow.list_run_infos(experiment_id=mlflow_experiment.experiment_id):\n",
    "    run = mlflow.get_run(run_info.run_id)\n",
    "    run_cv_metrics[run_info.run_id] = run.data.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look for the best run according to a given metric:\n",
    "metric_for_promotion = \"mean_test_roc_auc\"\n",
    "# # --Only keep the runs where the metric for promotion was logged:\n",
    "simplified_run_cv_metrics = {}\n",
    "for rid in run_cv_metrics.keys():\n",
    "    if metric_for_promotion in run_cv_metrics[rid].keys():\n",
    "        simplified_run_cv_metrics[rid] = run_cv_metrics[rid][metric_for_promotion]\n",
    "best_run_id = sorted(simplified_run_cv_metrics)[0]\n",
    "print(f\"The best run is {best_run_id} with a {metric_for_promotion} of {simplified_run_cv_metrics[best_run_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_run_cv_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve the path of its model directory\n",
    "best_run_info = mlflow.get_run(best_run_id).info\n",
    "model_path = f\"{mlflow_experiment.experiment_id}/{best_run_id}/artifacts/{model_algo}-{best_run_id}\"\n",
    "print(f\"Its model is located at: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.info.artifact_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's retrieve a handle for the Saved Model in which we are going to create our version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get or create the Saved Model\n",
    "sm_id = None\n",
    "for sm in project.list_saved_models():\n",
    "    if sm[\"name\"] != SAVED_MODEL_NAME:\n",
    "        continue\n",
    "    else:\n",
    "        sm_id = sm[\"id\"]\n",
    "        print(f\"Found Saved Model {sm['name']} with id {sm['id']}\")\n",
    "        break\n",
    "if sm_id:\n",
    "    sm = project.get_saved_model(sm_id)\n",
    "else:\n",
    "    sm = project.create_mlflow_pyfunc_model(name=SAVED_MODEL_NAME,\n",
    "                                            prediction_type=DSSPredictionMLTaskSettings.PredictionTypes.BINARY)\n",
    "    sm_id = sm.id\n",
    "    print(f\"Saved Model not found, created new one with id {sm_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now promote our experiment run and generate the corresponding Saved Model version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model by generating a Saved Model version\n",
    "#version_id = f\"{best_run_id}_{now_str()}\"\n",
    "#mlflow_version = sm.import_mlflow_version_from_managed_folder(version_id=version_id,\n",
    "#                                                              managed_folder=XP_TRACKING_FOLDER_ID,\n",
    "#                                                              path=model_path)\n",
    "# Make this Saved Model version the active one\n",
    "#sm.set_active_version(mlflow_version.version_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to be able to visualize the performance graphs of our newly-created Saved Model version, we need to *evaluate* it against an evaluation Dataset. In our case, it's called `uci_bank_evaluate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the imported model\n",
    "#mlflow_version.set_core_metadata(target_column_name=target_name,\n",
    "#                                 class_labels=[\"no\", \"yes\"], # TODO change with run params\n",
    "#                                 get_features_from_dataset=EVALUATION_DATASET)\n",
    "#mlflow_version.evaluate(EVALUATION_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now have a Saved Model version coming from a fully programmatic workflow!"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_SVKZNnoa",
  "createdOn": 1677262140587,
  "creationTag": {
   "lastModifiedBy": {
    "login": "kirsten.hoogenakker"
   },
   "lastModifiedOn": 1677262140587,
   "versionNumber": 0
  },
  "creator": "kirsten.hoogenakker",
  "customFields": {},
  "dkuGit": {
   "lastInteraction": 0
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env Python38)",
   "language": "python",
   "name": "py-dku-venv-python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "modifiedBy": "kirsten.hoogenakker",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
